{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de AprendizajeNoSupervisado.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ncadavia/DictamenJuridico/blob/master/NoSupervisado/AprendizajeNoSupervisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"p9xlC7y02sqU"},"source":["________________\n","# <center> Aprendizaje supervisado </center>\n","________________"]},{"cell_type":"markdown","metadata":{"id":"AzYDO__GRQy4"},"source":["<div align='justify'>\n","<h2> En el aprendizaje supervisado asumimos que existe una función $f$ entre las características $X$ y las etiquetas $Y$: \n","$$Y = f(X)$$\n","$f$ es desconocida y entonces los objetivos del aprendizaje supervisado se centran en encontrar un modelo $\\hat{f}$ que aproxime lo mejor posible a $f$: $$\\hat{f} \\approx f.$$\n","$\\hat{f}$ puede ser una regresión, un árbol de decisión, ...</h2>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"n_31tz17P8DV"},"source":["<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/Supervisado.png\">\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"tCDqHexD2i2d"},"source":["________________\n","# <center> Aprendizaje no supervisado </center>\n","________________"]},{"cell_type":"markdown","metadata":{"id":"wF3wfowQaDu4"},"source":["<h2> En el aprendizaje no supervisado contamos con las caracaterísticas $X$, pero no con las etiquetas $Y$. </h2>"]},{"cell_type":"markdown","metadata":{"id":"1-1LprZD5Fer"},"source":["<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/NoSupervisado.png\">\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"HjOwM4H7c1vD"},"source":["<div align='justify'>\n","<h2>El objetivo de un algoritmo de aprendizaje no supervisado es crear un modelo que tome las características $X$ como entrada y nos devuelva unas nuevas características o  un valor que se puede utilizar para resolver un problema práctico. Por ejemplo, en la <u><em>reducción de dimensionalidad</em></u>, la salida del modelo serán una cantidad de características inferior  a la entrada original $X$. En el <u><em> agrupamiento </em></u>, el modelo devuelve la identificación del grupo para cada observación en el conjunto de datos.</h2>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"ZDIPHWOWjvjY"},"source":["________________\n","# <center> Reducción de dimensionalidad </center>\n","________________\n"]},{"cell_type":"markdown","metadata":{"id":"28ox4gVPfT4W"},"source":["<div align='justify'>\n","<h2>\n","La reducción de dimensionalidad es el proceso de tomar datos en un espacio de alta dimensión y llevarlos a un nuevo espacio cuya dimensionalidad es mucho más pequeña.\n","</h2>\n","\n","<h2>\n","Hay varias razones para reducir la dimensionalidad de los datos:\n","</h2>\n","<h2>\n","$\\circ$ filtrar características no tan relevantes y conservar la mayor cantidad posible de las interesantes,\n","</h2>\n","<h2>\n","$\\circ$ los datos de alta dimensión imponen desafíos computacionales,\n","</h2>\n","<h2>\n","$\\circ$ la reducción de dimensionalidad es ideal para explorar y mejorar nuestra comprensión sobre un conjunto de datos.\n","</h2> \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"lfCpjEK1d68U"},"source":["## <center> <u> <h2> Análisis de componente principales ─ PCA </h2> </u> </center>"]},{"cell_type":"markdown","metadata":{"id":"m3NEKuFdSUTZ"},"source":["<h2> Algunas células\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/celulas.png\">\n","&emsp;\n","&emsp;\n","&emsp;\n","&emsp;\n","&emsp;\n","&emsp;\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/celulas2.png\">\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"qEZL0CvVSqqr"},"source":["<h2> Los datos </h2>\n","\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/tabla_celulas.png\">\n","</br>\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/grafica_celulas_1.png\">\n","\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/grafica_celulas_2.png\">\n","\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/grafica_celulas_3.png\">\n","</p>\n","\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/grafica_celulas_4.png\">\n","</p>\n","\n","<br>\n","<h2> <center> ¿Qué hacemos si hay 4 ó más células?</center></h2>\n","\n","<h2> Un gráfico de análisis de componentes principales convierte las correlaciones (o la falta de ellas) entre todas las células en un gráfico 2-D o 3-D.  </h2>\n","\n","\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/PCA.png\">\n","</p>\n","<div align='justify'>\n","<h2> Las células que tienen correlaciones elevadas se agrupan juntas.\n","<br>\n","Los ejes están clasificados por orden de importancia. Las diferencias a lo largo del primer eje de análisis de componentes principales (PC1) son más importantes que las diferencias a lo largo del segundo  eje de análisis de componentes principales (PC2).   </h2>\n","</div>\n","\n","\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/PCA2.png\">\n","</p>\n","\n","\n","[Ver video ](https://www.youtube.com/watch?v=HMOI_lkzW08)"]},{"cell_type":"markdown","metadata":{"id":"G-Ge1XrDiRBX"},"source":["<div align='justify'>\n","<h2>\n","<u> Un poco más de detalle:</u> las componentes principales son vectores que definen un nuevo sistema de coordenadas en el que el primer eje va en la dirección de la varianza más alta en los datos. El segundo eje es ortogonal al primero y va en la dirección de la segunda varianza más alta en los datos. El tercer eje sería ortogonal tanto al primer como al segundo eje e iría en la dirección de la tercera varianza más alta y así sucesivamente.\n","</h2>"]},{"cell_type":"markdown","metadata":{"id":"7Jr7NFAlpoKw"},"source":["## <center> <u> <h2> PCA en scikit-learn </h2> </u> </center>"]},{"cell_type":"markdown","metadata":{"id":"KjJOlkx9qxmx"},"source":["<div align='justify'>\n","<h2> scikit-learn tiene una implementación de PCA:\n","<br>\n","$\\circ$ el método <u>fit()</u> aprende cómo mover y rotar los datos para que estén alineados con los ejes coordenados y tengan media $0$\n","<br>\n","$\\circ$ el método <u>transform()</u> lleva a cabo la transformación que se aprendió con el método <u>fit()</u>,\n","<br>\n","$\\circ$ antes de usar PCA es importante estandarizar los datos \n","$$z = (x - u) / s$$\n","donde \n","<br>\n","$u$ es la media de las muestras de entrenamiento o cero si `with_mean = False`, \n","<br>\n","$s$ es la desviación estándar de las muestras de entrenamiento o uno si `with_std = False`,\n","<br>\n","Esto lo hacemos con `StandardScaler`\n","<br>   </h2>\n","<h2>\n","<strong> Pregunta:</strong>\n","¿Podremos usar PCA en muestras nuevas?\n","</h2>"]},{"cell_type":"code","metadata":{"id":"KB6hp96Q3ZDm","executionInfo":{"status":"ok","timestamp":1648523689791,"user_tz":300,"elapsed":500,"user":{"displayName":"Gabriel Quinche","userId":"07987137862424485285"}}},"source":["import pandas as pd "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayjRGG_O30Aa","executionInfo":{"status":"error","timestamp":1648523709164,"user_tz":300,"elapsed":531,"user":{"displayName":"Gabriel Quinche","userId":"07987137862424485285"}},"outputId":"a2012d6d-3f87-466a-dcf4-86974c498d2a","colab":{"base_uri":"https://localhost:8080/","height":396}},"source":["# Cada fila representa medidas para un pez individual entre ellas peso (g), \n","# largo (cm), altura (?), cocientre entre altura y largo,...\n","fish = pd.read_csv('fish.csv', header = None)\n","display(fish.sample(5))\n","print(\"\")\n","samples = fish.drop(columns=[0])\n","display(samples.sample(5))\n","\n","# Guardamos las especies en una nueva lista\n","especies = fish[0]\n","# especies.value_counts()\n","\n","# datos de http://jse.amstat.org/jse_data_archive.htm"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-27e6eb33306d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cada fila representa medidas para un pez individual entre ellas peso (g),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# largo (cm), altura (?), cocientre entre altura y largo,...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fish.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fish.csv'"]}]},{"cell_type":"code","metadata":{"id":"_Isv-L0leawr"},"source":["# Importando los paquetes necesario\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","import matplotlib.pyplot as plt\n","\n","# Estandarizar las características eliminando la media y escalando a la varianza \n","# de la unidad \n","scaler_1 = StandardScaler()\n","\n","# Creamos una instancia de PCA\n","pca_1 = PCA()\n","\n","# Creamos pipeline\n","pipeline_1 = make_pipeline(scaler_1, pca_1)\n","\n","# Ajustamos el pipeline a nuestros datos\n","pipeline_1.fit(samples)\n","\n","# Graficamos\n","features = range(pca_1.n_components_)\n","plt.bar(features, pca_1.explained_variance_)\n","plt.xlabel('Característica de PCA')\n","plt.ylabel('Varianza')\n","plt.xticks(features)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nz_WQhon0hnZ"},"source":["<div align='justify'>\n","<h2>\n","La dimensión intrínseca de los datos es el número de características de PCA con varianza significativa.\n","<br>\n","<strong> Pregunta:</strong>\n","Según la gráfica anterior sería razonable decir que la dimensión intrínseca del conjunto de datos es ¿1, 2, 3,...?\n","</h2>"]},{"cell_type":"markdown","metadata":{"id":"jQo00J57X57w"},"source":["<div align='justify'>\n","<h2>\n","¿Qué haremos en la siguiente celda de código?\n","<br>\n","Vamos a realizar una reducción de dimensionalidad, para ello necesitamos saber:\n","<br>\n","$\\circ$ ¿Cuántas características de PCA vamos a conservar? por ejemplo la dimensión intrínseca de los datos.\n","</h2>"]},{"cell_type":"code","metadata":{"id":"QLqYdf2Wx7Aj"},"source":["# Importando los paquetes\n","from sklearn.decomposition import PCA\n","\n","# Creamos una instancia de PCA: pca\n","pca_2 = PCA(n_components=2)\n","\n","# Creamos pipeline\n","pipeline_2 = make_pipeline(scaler_1, pca_2)\n","\n","# Ajustamos el pipeline a nuestros datos\n","pipeline_2.fit(samples)\n","\n","# Hallamos las características de PCA\n","pca_2_features = pipeline_2.transform(samples)\n","\n","# imprimimos \n","print(pca_2_features.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCsuXANOcVYZ"},"source":["# Aquí solo transformamos las categorías especies en categorías numéricas\n","# para usarlas en la siguiente grafica\n","especies_unicas = especies.unique()\n","dic_esp = {especies_unicas[k]:k for k in range(len(especies_unicas))}\n","species = especies.apply(lambda x: dic_esp[x])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SX_h5qxZA22"},"source":["import matplotlib.pyplot as plt\n","xs = pca_2_features[:,0]\n","ys = pca_2_features[:,1]\n","plt.scatter(xs, ys, c=species)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DaNnhaqTq8tK"},"source":["________________\n","# <center> Agrupamiento ─ Clustering </center>\n","________________"]},{"cell_type":"markdown","metadata":{"id":"G2S8As1Rq8-z"},"source":["<div align='justify'>\n","<h2>\n","$\\circ$ Una vez se han reducido el conjunto de características originales a un conjunto más pequeño y manejable, podemos encontrar patrones interesantes agrupando instancias similares de datos. \n","</h2>\n","<h2>\n","$\\circ$ El objetivo aquí es agrupar datos con base en su similitud: comparar cuán similar es una observación a otras observaciones. No se usan etiquetas.\n","</h2>\n","<h2>\n","$\\circ$ Así un <strong> grupo (cluster) </strong> en este contexto hace referencia a un conjunto de datos con características similares. \n","</h2>\n","<h2>\n","$\\circ$ El agrupamiento (clustering) se puede lograr con una variedad de algoritmos de aprendizaje no supervisados. Exploraremos los algoritmos <em> k-medias (k-means)</em> y <em>agrupamiento jerárquico (hierarchical clustering)</em>.\n","</h2>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Xjd7Xs1FNJdc"},"source":["<center> <h2> <u> k-medias ─ k-means </u> </h2> </center>\n"]},{"cell_type":"markdown","metadata":{"id":"eHS2rMbRTNRP"},"source":["<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/K-means.png\">\n","</p>\n","\n","\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/GraficaInercia.png\">\n","</p>\n"]},{"cell_type":"markdown","metadata":{"id":"iLQXBBh_N_Ye"},"source":["<div align='justify'>\n","<h2>\n","$\\circ$ El agrupamiento k-medias es un método para encontrar grupos y centros de grupos en un conjunto de datos sin etiquetar. Se elige el número deseado de centros de grupos. Dado un conjunto inicial de centros, el algoritmo alterna los pasos: \n","</h2>\n","<h2>\n","1. Para cada centro identificamos el subconjunto de puntos de entrenamiento (su grupo) que están más cerca de él que cualquier otro centro;\n","</h2>\n","<h2>\n","2. Se calcula el centro del grupo así: se calculan las medias de cada característica para los puntos de datos de cada grupo, y este vector de medias se convierte en el nuevo centro de ese grupo.\n","</h2>\n","<h2>\n","Estos dos pasos se repiten hasta la convergencia. Normalmente, los centros iniciales son observaciones elegidas aleatoriamente de los datos de entrenamiento. \n","</h2>\n","<h2>\n","$\\circ$ Diferentes ejecuciones de k-medias darán como resultado asignaciones de grupos ligeramente diferentes pues esta inicialización aleatoria es una fuente de aleatoriedad, lo que resulta en asignaciones de agrupamiento ligeramente diferentes, de una ejecución de k-medias a otra. \n","</h2>\n","\n","<h2>\n","$\\circ$ El valor de k, el número de grupos, es un hiperparámetro que debe ser ajustado por el analista de datos. Existen algunas técnicas para seleccionar k. Ninguno de ellas ha demostrado ser óptima. La mayoría requiere que el analista haga una \"suposición fundamentada\" al observar algunas métricas o al examinar visualmente las asignaciones de grupos.\n","</h2>\n","<h2>\n","$\\circ$ El algoritmo optimiza los grupos minimizando la variación dentro del grupo (también conocida como inercia) de modo que la suma de las variaciones dentro del grupo en todos los k grupos sea lo más pequeña posible.\n","</h2>\n","<h2>\n","$\\circ$ Un buen agrupamiento busca que no haya muchos grupos y que dentro de cada grupo los datos no estén muy dispersos. Para esto hacemos una gráfica de k's vs inercia, como la última figura.\n","</h2>\n","<h2>\n","$\\circ$ En K-medias la varianza de una característica corresponde a su influencia, por lo tanto debemos transformar las características para que tengan media $0$ y varianza $1$.\n","</h2>"]},{"cell_type":"markdown","metadata":{"id":"oT67toSmRlJy"},"source":["<h2>\n","<strong> Pregunta:</strong>\n","¿Podremos usar KMeans en datos nuevos?\n","</h2>"]},{"cell_type":"markdown","metadata":{"id":"7ISLdnUj5Y04"},"source":["<div align='justify'>\n","<h2>\n","¿Qué haremos en la siguiente celda de código?\n","<br>\n","Tenemos observaciones de medidas de peces. Cada fila representa un pez individual. Las medidas, como el peso en gramos, la longitud en centímetros y la relación porcentual de altura a longitud, tienen escalas muy diferentes. Para agrupar estos datos de manera eficaz, primero debemos estandarizar estas características para proceder a agrupar los datos. Usaremos un `pipeline` para hacer estos dos pasos.\n","</h2>"]},{"cell_type":"code","metadata":{"id":"eJK0qzi88kI7"},"source":["# Importando los paquetes\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","# Estandarizar las características: media 0 y varianza 1\n","scaler_2 = StandardScaler()\n","\n","# Creamos una instancia de kmeans con k=4\n","kmeans = KMeans(n_clusters=4)\n","\n","# Creamos pipeline, primero estandarizamos, luego agrupamos\n","pipeline_3 = make_pipeline(scaler_2, kmeans)\n","\n","# Ajustamos el pipeline a nuestros datos\n","pipeline_3.fit(samples)\n","\n","# Calculamos las etiquetas de los grupos\n","labels = pipeline_3.predict(samples)\n","\n","# Veamos cómo agrupo KMeans\n","labels_2 = pd.Series(labels)\n","print('Así agrupó nuestro algoritmo KMeans')\n","print(labels_2.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57bQkceiLVg3"},"source":["print('REALIDAD')\n","print(especies.value_counts())\n","print('')\n","print('KMeans')\n","print(labels_2.value_counts())\n","\n","# # Creamos un DataFrame con las etiquetas y especies como columnas\n","# df = pd.DataFrame({'KMeans': labels, 'REALIDAD': especies})\n","# print('')\n","# # Create crosstab\n","# ct = pd.crosstab(df['REALIDAD'], df ['KMeans'])\n","# print(\"\")\n","# print(ct)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mk1wxKEH8Hhp"},"source":["<div align='justify'>\n","<h2>\n","¿Qué haremos en la siguiente celda de código?\n","<br>\n","Recordemos que en un buen agrupamiento las observaciones dentro de cada grupo no están dispersas y que medimos cuan dispersas están las observaciones dentro de cada grupo por medio de la \"inercia\". \n","<br>\n","La inercia de un modelo KMeans es medida automáticamente cuando el método `fit` es usado y está disponible como un atributo.\n","</h2>"]},{"cell_type":"code","metadata":{"id":"2CnZV-3ORqNm"},"source":["# Diferentes valores de k, desde 1 hasta 5\n","ks = range(1,6)\n","\n","# En la lista inercia guardaremos la inercia para cada valor de k\n","inercias = []\n","\n","for k in ks:\n","  # Creamos una instancia de KMeans con k clusters\n","  model = KMeans(n_clusters = k)\n","\n","  # Ajustamos el modelo a los datos\n","  model.fit(samples)\n","  \n","  # Guardamos la inercia en la lista inercias\n","  inercias.append(model.inertia_)\n","\n","# Grafica de ks vs inercias\n","plt.plot(ks, inercias, '-o')\n","plt.xlabel('número de grupos, k') \n","plt.ylabel('inercia')\n","plt.xticks(ks)\n","plt.show()\n","\n","print(inercias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tS6Ad73ZNzOw"},"source":["<center> <h2> <u>Agrupamiento jerárquico ─ Hierarchical clustering</u></h2></center>\n","<center> <h3><u> Agrupamiento aglomerativo </u> </h3></center>"]},{"cell_type":"markdown","metadata":{"id":"5VfKosF4mNDW"},"source":["<div align='justify'>\n","<h2>\n","Una posible desventaja de la agrupación de K-medias es que requiere que pre-especifiquemos el número de agrupaciones K. La agrupación jerárquica es un enfoque alternativo que no requiere que nos comprometamos con una elección particular de K. La agrupación jerárquica tiene una ventaja adicional sobre K-medias porque da como resultado una representación atractiva de las observaciones basada en árboles, llamada dendrograma. \n","</h2>\n"]},{"cell_type":"markdown","metadata":{"id":"RFBWILj7mdbG"},"source":["<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/NoSupervisado/agrupamiento_jerarquico_aglomerativo.png\">\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"ulKu7a7yOAIO"},"source":["<div align='justify'>\n","<h2>\n","$\\circ$ Los métodos de agrupamiento jerárquico producen representaciones jerárquicas en las que los grupos en cada nivel de la jerarquía se crean fusionando grupos en el siguiente nivel inferior. En el nivel más bajo, cada grupo contiene una única observación. En el nivel más alto, solo hay un grupo que contiene todos los datos.\n","</h2>\n","\n","<h2>\n","$\\circ$ Las estrategias para la agrupación jerárquica se dividen en dos paradigmas básicos: aglomerativo (de abajo hacia arriba) y divisivo (de arriba hacia abajo). Nos centraremos en agrupamiento aglomerativo.\n","</h2>\n","\n","\n","<h2>\n","$\\circ$ Los algoritmos de agrupamiento aglomerativo comienzan con cada observación representado un grupo único. En cada uno de los N-1 pasos, los dos grupos más cercanos (menos disímiles) se fusionan en un solo grupo, produciendo un grupo menos en el siguiente nivel. Por lo tanto, se debe definir una medida de disimilitud entre dos grupos de observaciones. \n","</h2>\n","\n"]},{"cell_type":"code","metadata":{"id":"5oh1_H_IYbYy"},"source":["# Nuestros datos son sobre granos (contiene medidas como área, perímetro, \n","# longitud y varias otras) de muestras de grano\n","# https://archive.ics.uci.edu/ml/datasets/seeds\n","grains = pd.read_csv('grains.csv', header = None)\n","grains = grains.drop(columns=[7])\n","\n","# Creamos una lista que contiene cada fila de grain como una lista\n","ar = [list(grains.iloc[k].values) for k in range(len(grains))]\n","\n","#  La lista de varieties contiene la variedad de cada muestra de grano\n","varieties = pd.read_csv('varieties.csv', header = None)\n","varieties = varieties.drop(columns=[1])\n","\n","# Creamos una lista conteniendo el nombre de las variedad de cada observación\n","var = list(varieties[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWfGO9eujKCM"},"source":["# Importamos los paquetes necesarios\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","import matplotlib.pyplot as plt\n","\n","# Creamos una instancia de fog para que se vea mejor el dendrograma\n","fig = plt.subplots(1,1, figsize=(27,7))\n","\n","# Calculamos el enlace: mergings\n","mergings = linkage(ar, method = 'complete')\n","\n","# Graficamos el dendrograma usando las variedades como etiquetas\n","dendrogram(mergings, labels=var, leaf_rotation=90, leaf_font_size=10)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9sHE3ZEoou1"},"source":["<div align='justify'>\n","<h2>\n","El agrupamiento jerárquico no es solo una herramienta visual. Podemos extraer  los agrupamientos en estados intermedios del agrupamiento jerárquico. Las etiquetas de estos grupos intermedios pueden ser usados en otros calculos.\n","<br>\n","Un estado intermedio de un agrupamiento en este caso es especificado al escoger una altura en el dendograma. El eje y del dendrograma codifica la distancia entre grupos fusionados y la distancia entre grupos está definida por un método \"linkage method\".\n","<br>\n","`complete linkage method`: la distancia entre los grupos es  la máxima distancia entre sus observaciones.\n","<br>\n","Diferentes `linkage methodes` producen diferentes  agrupamientos jerárquicos\n","</h2>"]},{"cell_type":"markdown","metadata":{"id":"PhTwcpp1r0Q8"},"source":["<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/garestrear/ninja-pythonist/master/\n","NoSupervisado/linkage_methods.png\">\n","</p>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pZEf_-tIsEKF"},"source":["<div align='justify'>\n","<h2>\n","Las etiquetas de grupos  para cualquier estado intermedio en el agrupamiento jerárquico pueden ser extraídas usando la función `fcluster()` la cuál devuelve un arreglo en numpy de las etiquetas de los grupos.\n","</h2>"]},{"cell_type":"code","metadata":{"id":"trhoohpmN8jW"},"source":["# Importando los paquetes necesarios\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","\n","# Usando fcluster para extraer las etiquetas\n","labels_3 = fcluster(mergings, 6, criterion = 'distance')\n","\n","# Creando df con  etiquetas y variedades como columnas\n","df2 = pd.DataFrame({'etiquetas':labels_3, 'variedades': var})\n","\n","# Creando crosstab\n","ct2 = pd.crosstab(df2['etiquetas'], df2['variedades'])\n","\n","#mostrando ct2\n","print(ct2)"],"execution_count":null,"outputs":[]}]}